\
from __future__ import annotations

import shutil
from pathlib import Path

from src.tools.sql.generate_bulk_insert_sql import generate_dims_and_facts_bulk_insert_scripts
from src.tools.sql.generate_create_table_scripts import generate_all_create_tables
from src.utils.logging_utils import stage, skip, done


# ------------------------------------------------------------
# Repo utilities
# ------------------------------------------------------------

def _find_repo_root(start: Path) -> Path:
    """
    Find repo root by walking parents and looking for both 'src' and 'scripts'.
    """
    for p in [start, *start.parents]:
        if (p / "src").is_dir() and (p / "scripts").is_dir():
            return p
    return start.parents[4]


def _read_text(path: Path) -> str:
    """
    Simple reader for repo-owned SQL assets.
    We assume UTF-8; also accept UTF-8 BOM.
    """
    try:
        return path.read_text(encoding="utf-8-sig")
    except UnicodeDecodeError:
        # Last-resort fallback if a file was saved as UTF-16 by accident.
        return path.read_text(encoding="utf-16")


def _write_text(path: Path, text: str) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(text, encoding="utf-8", newline="\n")


def _sales_mode(sales_cfg: dict) -> str:
    mode = str(sales_cfg.get("sales_output") or "").strip().lower()
    return mode or "sales"


# ------------------------------------------------------------
# Static SQL assets
# ------------------------------------------------------------

def copy_static_sql_assets(*, sql_root: Path) -> None:
    repo_root = _find_repo_root(Path(__file__).resolve())
    cci_src = repo_root / "scripts" / "sql" / "columnstore" / "create_drop_cci.sql"
    if not cci_src.exists():
        skip(f"CCI script not found; skipping: {cci_src}")
        return

    dst_dir = sql_root / "indexes"
    dst_dir.mkdir(parents=True, exist_ok=True)
    shutil.copy2(cci_src, dst_dir / "create_drop_cci.sql")
    done("Copied CCI script")


def copy_views_sql(*, sql_root: Path) -> None:
    repo_root = _find_repo_root(Path(__file__).resolve())
    src = repo_root / "scripts" / "sql" / "views" / "create_views.sql"
    if not src.exists():
        skip(f"Views script not found; skipping: {src}")
        return

    dst = sql_root / "schema" / "04_create_views.sql"
    _write_text(dst, _read_text(src))
    done("Copied views script")


# ------------------------------------------------------------
# Constraints (mode-dependent)
# ------------------------------------------------------------

def compose_constraints_sql(*, sql_root: Path, sales_cfg: dict) -> None:
    """
    Compose <final>/sql/schema/03_create_constraints.sql from the modular constraint files.
    Falls back to legacy create_constraints.sql if modular parts are missing.
    """
    repo_root = _find_repo_root(Path(__file__).resolve())
    modular_dir = repo_root / "scripts" / "sql" / "bootstrap" / "constraints"
    legacy_file = repo_root / "scripts" / "sql" / "bootstrap" / "create_constraints.sql"

    out_path = sql_root / "schema" / "03_create_constraints.sql"
    mode = _sales_mode(sales_cfg)

    if modular_dir.exists() and modular_dir.is_dir():
        parts: list[Path] = [modular_dir / "00_dimensions.sql"]
        if mode in {"sales", "both"}:
            parts.append(modular_dir / "10_sales.sql")
        if mode in {"sales_order", "both"}:
            parts.extend(
                [
                    modular_dir / "20_sales_order_header.sql",
                    modular_dir / "21_sales_order_detail.sql",
                    modular_dir / "22_sales_order_relations.sql",
                ]
            )

        existing = [p for p in parts if p.exists()]
        if not existing:
            skip(f"No modular constraint parts found in: {modular_dir}; skipping constraints.")
            return

        chunks: list[str] = []
        chunks.append("-- Auto-generated by packaging: composed constraints\n")
        chunks.append(f"-- mode: {mode}\n")

        for p in existing:
            chunks.append("\n-- ============================================================\n" f"-- {p.name}\n" "-- ============================================================\n")
            chunks.append(_read_text(p).rstrip())

        _write_text(out_path, "\n".join(chunks).rstrip() + "\n")
        done("Composed constraints")
        return

    if legacy_file.exists():
        _write_text(out_path, _read_text(legacy_file))
        done("Copied legacy constraints")
        return

    skip("No constraints source found; skipping constraints")


# ------------------------------------------------------------
# SQL generation from packaged CSVs
# ------------------------------------------------------------

def write_create_table_scripts(*, dims_out: Path, facts_out: Path, sql_root: Path, cfg: dict) -> None:
    """
    CREATE TABLE scripts are generated from STATIC_SCHEMAS + cfg (not from CSV inspection),
    so we do NOT need to flatten facts or inspect filenames.
    """
    with stage("Generating CREATE TABLE Scripts"):
        dims_csv = list(dims_out.glob("*.csv"))
        facts_csv = list(facts_out.rglob("*.csv"))

        if not dims_csv and not facts_csv:
            skip("No CSV files found — skipping CREATE TABLE scripts.")
            return

        generate_all_create_tables(
            dim_folder=dims_out,
            fact_folder=facts_out,
            output_folder=sql_root,
            cfg=cfg,
        )


def write_bulk_insert_scripts(*, dims_out: Path, facts_out: Path, sql_root: Path, cfg: dict, **_) -> None:
    """
    Always generate:
      sql/load/01_bulk_insert_dims.sql
      sql/load/02_bulk_insert_facts.sql

    The facts allowlist is computed from the FULL cfg (sales_output + returns enablement).
    """
    with stage("Generating BULK INSERT Scripts"):
        dims_csv = list(dims_out.glob("*.csv"))
        facts_csv = list(facts_out.rglob("*.csv"))

        if not dims_csv and not facts_csv:
            skip("No CSV files found — skipping BULK INSERT scripts.")
            return

        load_root = sql_root / "load"
        generate_dims_and_facts_bulk_insert_scripts(
            dims_folder=str(dims_out),
            facts_folder=str(facts_out),
            cfg=cfg,
            load_output_folder=str(load_root),
            dims_mode="csv",
            facts_mode="legacy",
            row_terminator="0x0a",
        )
